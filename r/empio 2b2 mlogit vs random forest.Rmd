---
title: 'Empirical Industrial Organisation 2b-Part 2: Comparing Out-Of-Sample Prediction
  Accuracy of a Multinomial Logit Model and a Random Forest'
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, error=TRUE)
library(dplyr)
library(mlogit)
library(tidyr)
library(restorepoint)
library(ggplot2)
```

Previously in the lecture, we analysed a data set about heating choices using a multinomial logit model, that allows us to estimate utility functions for heating systems for different households.

In a similar spirit as in your exercise classes about machine learning, we now want to analyse, how well our multinomial logit model predicts out-of-sample compared to a random forest. Random forest is a very popular machine learning method that often yields very good out-of-sample prediction accuracy.


a\) Load the dataset Heating from the package mlogit. Split it into a training data set (2/3 of rows) and a test data set (1/3 of rows)

```{r}
# Load data
library("mlogit")
data("Heating", package = "mlogit")
dat = Heating

# Set a random seed for reproducibility
set.seed(123456789)

# Choose 600 rows for training
train.ind = sample.int(900,600,replace = FALSE)

# Create training and test data frames
train = dat[train.ind,]
test = dat[-train.ind,]
```

b\) Estimate a multinomial logit model, with alternative specific constants and investment and operation cost as explanatory variables on the training data set.
```{r}
# First we transform the training data set
# into the long format used by the 
# mlogit function 
train.long = mlogit.data(train, shape="wide", choice="depvar", varying=c(3:12))

# Estimate the mlogit model on the
# training data
ml <- mlogit(depvar~ic+oc, train.long, reflevel = "hp")
# Show a summary of the results
summary(ml)

```

c\) Using the test data set predict choice probabilities for each heating system based on our model estimated with the training data. We will analyse and compare the predictions with the random forest later. 

```{r}
# Transform test data set into required
# format
test.long = mlogit.data(test, shape="wide", choice="depvar", varying=c(3:12))

# Predict on test data
ml.pred = predict(ml,test.long)
# Show first 3 rows
ml.pred[1:3,]
```

d\) Now use the package `ranger` to train a random forest that predicts choice probabilities for the heating system on our training data set. We don't tune the parameters of the random forest, but take the default parameters. You can use all explanatory variables that could be relevant for a households choice.
Afterwards compute predicted choice probabilities for the test data set. 
```{r}
library(ranger)


# Use all columns except for idcase to 
# predict heating choice
rf = ranger(depvar ~ . - idcase, train, probability = TRUE)
rf

# Compute predicted probabilities
# for test data set
rf.pred = predict(rf,test)$prediction
rf.pred[1:3,]


rf.pred = rf.pred[, colnames(ml.pred)]
rf.pred[1:3,]
```

e\) The following code creates data frame of prediction probailities in a long format that will be suited for later analysis with dplyr and ggplot2. Try to understand step by step, what the following code does.
```{r}
library(dplyr)
library(tidyr)

pred.wide = rbind(
  cbind(data.frame(model="mlogit", choice=test$depvar), ml.pred),
  cbind(data.frame(model="rf",choice=test$depvar), rf.pred)
)
pred.wide[1:3,]

pred = pred.wide %>%
  gather(key="option",value="prob", ec, er, gc, gr, hp) %>%
  arrange(model,  option)
pred[1:3,]

cpred = filter(pred, option==choice)
cpred[1:3,]
```

f\) Use the dplyr functions `group_by` and `summarize` to compute the mean predicted probability of the actual chosen heating system in the test data set for the mlogit and the (not tuned) random forest. Which model has better out-of-sample accuracy according to this measure? Also show this measure separately for each heating system.
```{r}
# Average predicted probability for the actually
# chosen heating system
cpred %>% group_by(model) %>%
  summarize(mean.prob = mean(prob))

# Separately for each heating system
cpred %>% group_by(model,choice) %>%
  summarize(mean.prob = mean(prob)) %>%
  spread(model, mean.prob)
```

g\) Compare graphically the histograms of predicted probabilities of the chosen alternative for both models. Use the ggplot2 package.

```{r}
library(ggplot2)

ggplot(cpred, aes(x=prob, group=model, fill=model)) +
  geom_histogram(alpha=0.7,binwidth = 0.025, position = "identity")

# Separately for each heating system
ggplot(cpred, aes(x=prob, group=model, fill=model)) +
  geom_histogram(alpha=0.7,binwidth = 0.025, position = "identity") +
  facet_wrap(~choice)
```

h\) (Optional) In the exercise classes on machine learning, you learned how one can use cross-validation for parameter tuning. Another application of cross validation is to repeat the whole procedure to estimate models on the training data set and predict it on a test data set, for different folds for test and training data.

This allows for better estimates of the out-of-sample prediction accuracy, but it takes longer. Take a look at the code below that implements this procedure.

```{r}
library(restorepoint)
est.and.pred = function(train, test, fold=0) {
  restore.point("est.and.pred")
  
  # Estimate mlogit on train data
  train.long = mlogit.data(train, shape="wide", choice="depvar", varying=c(3:12))
  ml <- mlogit(depvar~ic+oc | income, train.long)
  
  # Predict mlogit on test data
  test.long = mlogit.data(test, shape="wide", choice="depvar", varying=c(3:12))
  ml.pred = predict(ml,test.long)

  # Estimate random forest on train data
  rf = ranger(depvar ~ . - idcase, train, probability = TRUE)
  
  # Predicted probabilities
  # for random forest on test data
  rf.pred = predict(rf,test)$prediction
  rf.pred = rf.pred[, colnames(ml.pred)]

  # Create prediction data set
  # in a nice long format
  pred.wide = rbind(
    cbind(data.frame(model="mlogit", choice=test$depvar), ml.pred),
    cbind(data.frame(model="rf",choice=test$depvar), rf.pred)
  )
  pred = pred.wide %>%
    gather(key="option",value="prob", ec, er, gc, gr, hp)
  
  # Only return probabilities for
  # actually chosen options
  cpred = filter(pred, option==choice) %>%
    mutate(fold=fold)
  cpred
}

# Create k=3 folds
k = 3
# Assign a random fold to each row
folds = sample.int(900) %% 3 +1
head(folds)
table(folds)

# Let each of the k-folds be the test data set and repeat or procedure above
li = lapply(1:k, function(fold) {
  cat("\nfold ", fold)
  
  train = dat[folds != fold,]
  test = dat[folds == fold,]
  cpred = est.and.pred(train, test, fold)
  cpred
})

# Combine the returned predictions of all 
# folds
cpred = bind_rows(li)

# Average predicted probability for the actually
# chosen heating system
cpred %>% group_by(model) %>%
  summarize(mean.prob = mean(prob))

# Separately for each heating system
cpred %>% group_by(model,choice) %>%
  summarize(mean.prob = mean(prob)) %>%
  spread(model, mean.prob)

# Graphical analysis
ggplot(cpred, aes(x=prob, group=model, fill=model)) +
  geom_histogram(alpha=0.7,binwidth = 0.025, position = "identity")

# Separately for each heating system
ggplot(cpred, aes(x=prob, group=model, fill=model)) +
  geom_histogram(alpha=0.7,binwidth = 0.025, position = "identity") +
  facet_wrap(~choice)


```


i\) (Optional) Let us take a look at the variable importance of the random forest model.
```{r}
rf = ranger(depvar ~ . - idcase, train, importance = "permutation",probability = TRUE)
rf

# Show variable importance
imp.rf =  sort(importance(rf),decreasing = TRUE)
imp.rf

# Show a plot
par(las=2) # make label text perpendicular to axis
barplot(rev(imp.rf), horiz=TRUE)
```

